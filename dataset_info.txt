```
2.  **`criteo_dataset.py`**:
    *   **Purpose**: Defines the `CriteoDataset` class to read and parse the Criteo competition data files (`.txt.gz` or `.txt`).
    *   **Working**:
        *   It can handle both gzipped and plain text files.
        *   It reads the file line by line but groups lines belonging to the *same impression* together. An impression consists of one or more candidate ads shown to a user at the same time.
        *   The `isTest` flag tells it whether to expect `cost` (click/no-click label) and `propensity` information, which are only present in training/validation data, not the final test set.
        *   The `inverse_propensity` flag handles whether the propensity score in the file is stored directly or as its inverse (1/propensity).
        *   It iterates through the dataset, yielding one dictionary per impression. Each dictionary contains:
            *   `id`: The unique impression ID.
            *   `candidates`: A list of dictionaries, where each inner dictionary represents the features of one candidate ad in that impression (using `utils.extract_features`).
            *   `cost` (if not `isTest`): The label (e.g., 0.001 for click, 0.999 for no-click).
            *   `propensity` (if not `isTest`): The propensity score for the logged action (adjusted based on `inverse_propensity`).
        *   It uses helper functions from utils.py extensively for parsing specific parts of each line.

3.  **`criteo_prediction.py`**:
    *   **Purpose**: Defines the `CriteoPrediction` class to read prediction files generated by a model.
    *   **Working**:
        *   Reads prediction files (potentially gzipped) line by line. Each line corresponds to one impression.
        *   Expected format per line: `impression_id;action0:score0,action1:score1,...` where `action` is the index of the candidate ad within the impression and `score` is the model's score for that candidate.
        *   It parses each line into a dictionary containing:
            *   `id`: The impression ID.
            *   `scores`: A NumPy array where the index corresponds to the candidate action and the value is the predicted score.
        *   It also counts the total number of predictions (`max_instances`) in the file upon initialization.

4.  **`compute_score.py`**:
    *   **Purpose**: Calculates the official competition score (IPS and SNIPS) based on a prediction file and a ground truth (gold labels) file.
    *   **Working**:
        *   Takes paths to the prediction file and the gold labels file.
        *   Uses `CriteoDataset` to read the gold labels (with `cost` and `propensity`) and `CriteoPrediction` to read the submitted predictions.
        *   Iterates through both files simultaneously, ensuring impression IDs match.
        *   For each impression:
            *   Gets the predicted scores for each candidate from the prediction file.
            *   Gets the actual `cost` (click/no-click, converted to `rectified_label` 0 or 1) and `propensity` from the gold labels file.
            *   Calculates an `prediction_stochastic_weight` for the impression. This weight is crucial for off-policy evaluation. It's calculated using the probability of the *logged action* (the action actually taken in the dataset) according to the *submitted policy* (derived from the prediction scores), divided by the `propensity` score (probability of the logged action according to the *original logging policy*).
            *   Accumulates weighted labels (`prediction_stochastic_numerator`) and the weights themselves (`prediction_stochastic_denominator`).
        *   **Evaluation Metrics**:
            *   **IPS (Inverse Propensity Score)**: Calculated as the sum of `prediction_stochastic_numerator` divided by a modified denominator based on the total weighted number of positive and negative instances in the gold set. This estimates the expected reward if the submitted policy were deployed.
            *   **SNIPS (Self-Normalized IPS)**: Calculated as the sum of `prediction_stochastic_numerator` divided by the sum of `prediction_stochastic_denominator`. This is often more stable than IPS as it normalizes by the sum of importance weights.
        *   It also calculates the standard error for IPS, SNIPS, and the average Importance Weight (ImpWt) using statistical formulas (including the Delta Method for the SNIPS ratio) to provide confidence intervals.
        *   Returns a dictionary containing these calculated scores and their standard errors.

5.  **`generate_random_predictions.py`**:
    *   **Purpose**: Creates a baseline prediction file where scores are assigned randomly.
    *   **Working**:
        *   Takes the path to a test set file (using `CriteoDataset` with `isTest=True`) and an output path.
        *   For each impression in the test set:
            *   It determines the number of candidates.
            *   The `_policy` function generates a random score (between 0 and 10) for each candidate using `np.random.rand()`.
            *   It formats the impression ID and the random scores into the required `impression_id;action0:score0,...` format.
            *   Writes the formatted line to the gzipped output file.

6.  **`parser_example.py`**:
    *   **Purpose**: Demonstrates how to use `CriteoDataset` to read and access data.
    *   **Working**:
        *   Creates a `CriteoDataset` instance for a *training* file (where `cost` and `propensity` exist).
        *   Iterates through the dataset and simply prints the dictionary structure returned for each impression, showing the `id`, `cost`, `propensity`, and the list of `candidates` with their features.

7.  **`submit_random_predictions.py`**:
    *   **Purpose**: Submits a generated prediction file to the CrowdAI platform for the competition.
    *   **Working**:
        *   Uses the `crowdai` library.
        *   Takes the prediction file path and the user's CrowdAI API key as arguments.
        *   Connects to the specific "CriteoAdPlacementNIPS2017" challenge.
        *   Calls `challenge.submit()` to upload the prediction file (specifying if it's for the small test set or the full one).
        *   Prints the response from the CrowdAI server, which typically includes the calculated scores (IPS, SNIPS, etc.).

8.  **`utils.py`**:
    *   **Purpose**: Contains low-level utility functions for parsing the specific text format of the Criteo dataset lines.
    *   **Working**:
        *   `extract_impression_id`: Gets the string before the first `|`.
        *   `extract_cost_propensity`: Parses the values after `|l` and `|p`.
        *   `extract_features`: Parses the feature string after `|f`, splitting by space and then colon to create a dictionary of `{feature_index: feature_value}`.
        *   `dump_feature`, `dump_impression`: Functions to convert the parsed dictionary representations back into the original Criteo text format (useful if modifying and rewriting data).
        *   `compute_integral_hash`: A specific hashing function used in compute_score.py (when `salt_swap` is enabled) likely to ensure consistent tie-breaking or selection based on impression ID.
```