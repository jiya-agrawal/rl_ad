{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 1024 examples\n",
      "Processed batch with 435 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 1024 examples\n",
      "Processed test batch with 784 examples\n"
     ]
    }
   ],
   "source": [
    "def parse_criteo_data(file_path, batch_size=1024):\n",
    "    \"\"\"\n",
    "    A generator-based parser for the Criteo dataset that yields mini-batches of parsed data.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file (.txt).\n",
    "        batch_size (int): Number of examples per mini-batch.\n",
    "    \n",
    "    Yields:\n",
    "        list: A list of dictionaries containing parsed features, labels, and propensity scores for a mini-batch.\n",
    "    \"\"\"\n",
    "    def parse_header_line(line):\n",
    "        \"\"\"\n",
    "        Parses the header line of an example.\n",
    "        \n",
    "        Args:\n",
    "            line (str): A single header line from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Parsed header information (example ID, label, propensity, display features).\n",
    "        \"\"\"\n",
    "        parts = line.strip().split()\n",
    "        example_id = parts[0]\n",
    "        # print(f\"Example ID: {example_id}\")\n",
    "        # Initialize variables\n",
    "        label = None\n",
    "        propensity = None\n",
    "        display_features = {}\n",
    "        # print(f\"Parts: {parts}\")\n",
    "        # Iterate through parts to extract label, propensity, and features\n",
    "        i = 1  # Start from index 1 since index 0 is the example ID\n",
    "        while i < len(parts):\n",
    "            part = parts[i]\n",
    "            if part.startswith(\"|l\"):\n",
    "                # Label is the next element after \"|l\"\n",
    "                label = float(parts[i + 1])\n",
    "                i += 2  # Skip the next element since it's already processed\n",
    "            elif part.startswith(\"|p\"):\n",
    "                # Propensity is the next element after \"|p\"\n",
    "                propensity = float(parts[i + 1].split(\"|\")[0])\n",
    "                i += 2  # Skip the next element since it's already processed\n",
    "            elif \":\" in part:\n",
    "                # Parse display features (|f)\n",
    "                # print(f\"Pending part {part}\")\n",
    "                key, value = part.split(\":\")\n",
    "                display_features[int(key)] = float(value)\n",
    "                i += 1\n",
    "            else:\n",
    "                # Handle unexpected format\n",
    "                print(f\"Unexpected part: {part}\")\n",
    "                i += 1\n",
    "        \n",
    "        return {\n",
    "            \"example_id\": example_id,\n",
    "            \"label\": label,\n",
    "            \"propensity\": propensity,\n",
    "            \"display_features\": display_features,\n",
    "            \"product_features\": []  # Placeholder for product features\n",
    "        }\n",
    "    \n",
    "    def parse_product_line(line):\n",
    "        \"\"\"\n",
    "        Parses a product line of an example.\n",
    "        \n",
    "        Args:\n",
    "            line (str): A single product line from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Parsed product features.\n",
    "        \"\"\"\n",
    "        parts = line.strip().split()\n",
    "        example_id = parts[0]\n",
    "        \n",
    "        # Parse product features (|f)\n",
    "        product_features = {}\n",
    "        # print(f\"Product line parts: {parts}\")\n",
    "        for feat in parts[2:]:\n",
    "            # print(f\"Pending feature {feat}\")\n",
    "            key, value = feat.split(\":\")\n",
    "            product_features[int(key)] = float(value)\n",
    "        \n",
    "        return product_features\n",
    "    \n",
    "    def read_file():\n",
    "        \"\"\"Generator to read the file line by line.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "    \n",
    "    # Initialize variables for batching\n",
    "    current_batch = []\n",
    "    header = None\n",
    "    \n",
    "    for line in read_file():\n",
    "        if line.startswith(\"|\"):\n",
    "            continue  # Skip malformed lines\n",
    "        \n",
    "        if \"|l\" in line and \"|p\" in line:\n",
    "            # Header line\n",
    "            if header is not None:\n",
    "                # If there's an incomplete example, skip it\n",
    "                header = None\n",
    "            \n",
    "            header = parse_header_line(line)\n",
    "        else:\n",
    "            # Product line\n",
    "            if header is not None:\n",
    "                product_features = parse_product_line(line)\n",
    "                header[\"product_features\"].append(product_features)\n",
    "            \n",
    "            # Yield the batch if we've collected all products for this example\n",
    "            if header and len(header[\"product_features\"]) == len(header[\"product_features\"]):\n",
    "                current_batch.append(header)\n",
    "                header = None  # Reset header for the next example\n",
    "                \n",
    "                # Yield the batch if it reaches the desired size\n",
    "                if len(current_batch) == batch_size:\n",
    "                    yield current_batch\n",
    "                    current_batch = []\n",
    "    \n",
    "    # Yield any remaining examples in the last batch\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "# Path to the dataset file\n",
    "train_data_path = \"criteo_train_small.txt/criteo_train_small.txt\"\n",
    "test_data_path = \"criteo_test_release_small.txt/criteo_test_release_small.txt\"\n",
    "\n",
    "# Create a parser for the training data\n",
    "train_parser = parse_criteo_data(train_data_path, batch_size=1024)\n",
    "\n",
    "# Iterate through mini-batches\n",
    "for batch in train_parser:\n",
    "    # Process the batch (e.g., train your epsilon-greedy model)\n",
    "    print(f\"Processed batch with {len(batch)} examples\")\n",
    "\n",
    "\n",
    "def parse_criteo_test_data(file_path, batch_size=1024):\n",
    "    \"\"\"\n",
    "    A generator-based parser for the Criteo test dataset that yields mini-batches of parsed product features.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the test dataset file (.txt).\n",
    "        batch_size (int): Number of examples per mini-batch.\n",
    "    \n",
    "    Yields:\n",
    "        list: A list of lists containing parsed product features for each example in the mini-batch.\n",
    "    \"\"\"\n",
    "    def parse_product_line(line):\n",
    "        \"\"\"\n",
    "        Parses a product line of an example.\n",
    "        \n",
    "        Args:\n",
    "            line (str): A single product line from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Parsed product features.\n",
    "        \"\"\"\n",
    "        parts = line.strip().split()\n",
    "        example_id = parts[0]\n",
    "        \n",
    "        # Parse product features (|f)\n",
    "        product_features = {}\n",
    "        # print(f\"Product line parts: {parts}\")\n",
    "        for feat in parts[2:]:\n",
    "            # print(f\"Pending feature {feat}\")\n",
    "            key, value = feat.split(\":\")\n",
    "            product_features[int(key)] = float(value)\n",
    "        \n",
    "        return product_features\n",
    "    \n",
    "    def read_file():\n",
    "        \"\"\"Generator to read the file line by line.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "    \n",
    "    # Initialize variables for batching\n",
    "    current_batch = []\n",
    "    current_example = []\n",
    "    current_example_id = None\n",
    "    \n",
    "    for line in read_file():\n",
    "        if line.startswith(\"|\"):\n",
    "            continue  # Skip malformed lines\n",
    "        \n",
    "        # Parse the product line\n",
    "        parts = line.strip().split()\n",
    "        example_id = parts[0]\n",
    "        \n",
    "        if example_id != current_example_id:\n",
    "            # If we've started a new example, finalize the previous one\n",
    "            if current_example:\n",
    "                current_batch.append(current_example)\n",
    "                if len(current_batch) == batch_size:\n",
    "                    yield current_batch\n",
    "                    current_batch = []\n",
    "            \n",
    "            # Start a new example\n",
    "            current_example_id = example_id\n",
    "            current_example = []\n",
    "        \n",
    "        # Parse the product features and add them to the current example\n",
    "        product_features = parse_product_line(line)\n",
    "        current_example.append(product_features)\n",
    "    \n",
    "    # Yield any remaining examples in the last batch\n",
    "    if current_example:\n",
    "        current_batch.append(current_example)\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "test_parser = parse_criteo_test_data(test_data_path, batch_size=1024)\n",
    "\n",
    "for batch in test_parser:\n",
    "    # Process the batch (e.g., evaluate your model)\n",
    "    print(f\"Processed test batch with {len(batch)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fd836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
